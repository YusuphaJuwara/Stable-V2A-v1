{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Inference Notebook used to test Stable-Foley on 5 random videos taken from the Greatest Hits Dataset test set. \n",
    "Follow the instructions provided as comments throughout the notebook. You will need to change the paths where specified and indicate the device that will be used by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import hydra\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "!pip install stable_audio_tools                                                                     \n",
    "from stable_audio_tools.inference.generation import generate_diffusion_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/christian/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Specify your token here. \n",
    "# (You can get it from https://huggingface.co/stabilityai/stable-audio-open-1.0 by loging in and accepting terms)\n",
    "\n",
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GreatestHits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/stablefoley/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/opt/anaconda3/envs/stablefoley/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'flash_attn'\n",
      "flash_attn not installed, disabling Flash Attention\n",
      "Initalize Stage1 CAVP Model\n",
      "Loading Stage1 CAVP Model from: /home/christian/syncfusion/Stable-Video2Audio/logs/cavp_ckpt/cavp_epoch66.ckpt\n",
      "Restored from /home/christian/syncfusion/Stable-Video2Audio/logs/cavp_ckpt/cavp_epoch66.ckpt with 0 missing and 0 unexpected keys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/stablefoley/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Loading test dataset: 196it [01:06,  2.93it/s]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Greatesthit test dataset:\n",
      "num test chunks: 602\n",
      "chunk frames size: torch.Size([3, 40, 224, 224])\n",
      "chunk audio size: torch.Size([2, 441000])\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "\n",
    "# We will need Stable-Foley, CAVP cand CLAP checkpoints to generate audio.\n",
    "# You can place downloaded checkpoints in the following paths:\n",
    "# CAVP ckpt: 'Stable-Video2Audio/logs/cavp_ckpt/cavp_epoch66.ckpt'\n",
    "# CLAP ckpt: '/homes/rfg543/Documents/Stable-Video2Audio/logs/clap_ckpt/630k-audioset-best.pt'\n",
    "# Stable-Foley ckpt: 'Stable-Video2Audio/logs/ckpts/gh-controlnet_2024-08-22-11-43-14/epoch=213-valid_loss=0.493.ckpt' \n",
    "\n",
    "\n",
    "seed = 1234\n",
    "num_samples = 12 # number of samples to generate (in this case 2 outputs will be generated)\n",
    "sample_duration = 10\n",
    "sr = 44100\n",
    "exp_cfg = \"train_gh_controlnet\"\n",
    "use_cavp = True\n",
    "promt_type = \"audio\" # \"audio\" or \"text\"\n",
    "dataset_path = \"/home/christian/mic-mp4-processed\" \n",
    "\n",
    "rms_from_target = True  # if True, the RMS envelope will be derived from the target audio\n",
    "                        # if False, the RMS is loaded from those generated in Stage 1   \n",
    "\n",
    "# If rms_from_target is False, this variable will be used to interpolate the RMS envelope\n",
    "target_length = sr*sample_duration\n",
    "\n",
    "if use_cavp:\n",
    "    ckpt_path = \"/home/christian/syncfusion/Stable-Video2Audio/logs/stablev2a/ckpts/gh-controlnet_2024-10-17-18-22-03/epoch=510-valid_loss=0.493.ckpt\"\n",
    "else:\n",
    "    ckpt_path = \"/home/christian/syncfusion/Stable-Video2Audio/logs/stablev2a/ckpts/gh-controlnet_2024-08-24-09-55-07_nocavp/epoch=213-valid_loss=0.493.ckpt\"\n",
    "# dataset_path = \"\"\n",
    "\n",
    "torch.cuda.set_device(0) # set the GPU to use\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "\n",
    "# load config: This is the config file that was used to train the model, it is needed to load the model and the dataloader\n",
    "with hydra.initialize(config_path=\"..\", version_base=None):\n",
    "    cond_cfg = hydra.compose(config_name=\"config\", overrides=[f'exp={exp_cfg}',\n",
    "                                                              f\"datamodule.root_dir=/home/christian/mic-mp4-processed\",  # You can override some parameters here, if needed\n",
    "                                                              f\"datamodule.test_split_file_path=/home/christian/syncfusion/test.txt\",\n",
    "                                                              f\"datamodule.test_data_to_use=1.0\",])\n",
    "    \n",
    "# init model\n",
    "model = hydra.utils.instantiate(cond_cfg[\"model\"])\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt['state_dict'], strict=False)\n",
    "model = model.cuda()\n",
    "# load dataloader\n",
    "datamodule = hydra.utils.instantiate(cond_cfg[\"datamodule\"])\n",
    "datamodule.setup(stage=\"test\")\n",
    "test_dataloader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.module_controlnet import window_rms, low_pass_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# window sizes for RMS and low-pass filter, these values ensure target RMS has the same shape as the audio\n",
    "# (this is required for the ControlNet to work properly)\n",
    "# low-pass filter is used to smooth the envelope\n",
    "rms_window_size = 10000\n",
    "low_pass_window_size = 2000\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "prima_iterazione = True\n",
    "iter_dati = iter(test_dataloader)\n",
    "dati = next(iter_dati)\n",
    "contatore = 0\n",
    "pbar = tqdm(total=len(test_dataloader)*num_samples)\n",
    "\n",
    "out_folder = \"out21_audiorms_nocavp_150step\"\n",
    "\n",
    "while dati:\n",
    "    if exp_cfg == \"train_gh_controlnet\":\n",
    "        x, frames, seconds_start, seconds_total, chunk_rms, item = dati\n",
    "    else:\n",
    "        x, frames, seconds_start, seconds_total, item = dati\n",
    "\n",
    "    if rms_from_target:\n",
    "        # rms envelope is derived from the target audio\n",
    "        rms_envelope = window_rms(x, window_size=rms_window_size)\n",
    "        filtered_envelope = low_pass_filter(rms_envelope, window_size=low_pass_window_size)\n",
    "    else:\n",
    "        # rms da decoficare e poi provare a interpolare per raggiungere 441000\n",
    "        chunk_rms = chunk_rms.cpu().numpy()\n",
    "        mu_expanded = librosa.mu_expand(chunk_rms, mu=127, quantize=True)\n",
    "        interval_original = np.linspace(0, 1, mu_expanded.shape[-1])\n",
    "        interval_target = np.linspace(0, 1, target_length)\n",
    "        filtered_envelope = np.array([interp1d(interval_original, elem, axis=-1)(interval_target) for elem in mu_expanded])\n",
    "        filtered_envelope = np.repeat(filtered_envelope[:, None, :], 2, axis=1)\n",
    "        filtered_envelope = torch.tensor(filtered_envelope, dtype=torch.float32)\n",
    "\n",
    "    num_samples = min(num_samples, x.shape[0])\n",
    "\n",
    "    conditionings = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        curr_conditioning = {\n",
    "            \"envelope\": filtered_envelope[i:i+1].to(device), # ControlNet input\n",
    "            \"seconds_start\": seconds_start[i:i+1], # start time\n",
    "            \"seconds_total\": seconds_total[i:i+1], # total time\n",
    "        }\n",
    "        if use_cavp:\n",
    "            curr_conditioning[\"frames\"] = frames[i:i+1] # frames CAVP embeddings (cross-attention)\n",
    "        if promt_type == \"audio\":\n",
    "            curr_conditioning[\"audio\"] = x[i:i+1] # audio CLAP embeddings (cross-attention)\n",
    "        elif promt_type == \"text\":\n",
    "            curr_conditioning[\"text\"] = item[i][\"text\"] # text CLAP embeddings (cross-attention)\n",
    "\n",
    "        conditionings.append(curr_conditioning)\n",
    "\n",
    "    if prima_iterazione:\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.plot(filtered_envelope[0, 0].cpu())\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        prima_iterazione = False\n",
    "\n",
    "    output = generate_diffusion_cond(\n",
    "            model.model,\n",
    "            seed=seed,\n",
    "            batch_size=num_samples,\n",
    "            steps=150,\n",
    "            cfg_scale=2.0,\n",
    "            conditioning=conditionings,\n",
    "            sample_size=x.shape[-1],\n",
    "            sigma_min=0.3,\n",
    "            sigma_max=500,\n",
    "            sampler_type=\"dpmpp-3m-sde\",\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "\n",
    "    if out_folder not in os.listdir():\n",
    "        os.mkdir(f\"{out_folder}\")\n",
    "\n",
    "    if not os.path.exists(f\"{out_folder}/input\"):\n",
    "        os.mkdir(f\"{out_folder}/input\")\n",
    "    \n",
    "    if not os.path.exists(f\"{out_folder}/output\"):\n",
    "        os.mkdir(f\"{out_folder}/output\")\n",
    "        \n",
    "\n",
    "    for i in range(num_samples):\n",
    "        video_name = item[i][\"video_name\"]\n",
    "        torchaudio.save(f\"{out_folder}/input/{contatore:04d}_video_{video_name}.wav\", x[i].cpu(), sample_rate=44100)\n",
    "        torchaudio.save(f\"{out_folder}/output/{contatore:04d}_video_{video_name}.wav\", output[i].cpu(), sample_rate=44100)\n",
    "        contatore += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    dati = next(iter_dati)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis, metrics, support code\n",
    "\n",
    "Not yet structured to be user-friendly, it should generally work if files are named in a standard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will plot the spectrogram of the target audio and the generated audio so we can compare them\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "audio_path1 = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out/input_0_video_2015-02-16-16-49-06.wav\" \n",
    "audio_path2 = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out/output_0_video_2015-02-16-16-49-06.wav\"\n",
    "\n",
    "y1, sr1 = librosa.load(audio_path1)\n",
    "y2, sr2 = librosa.load(audio_path2)\n",
    "\n",
    "D1 = librosa.amplitude_to_db(np.abs(librosa.stft(y1)), ref=np.max)\n",
    "D2 = librosa.amplitude_to_db(np.abs(librosa.stft(y2)), ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.specshow(D1, sr=sr1, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram of Target Audio')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "librosa.display.specshow(D2, sr=sr2, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram of Generated Audio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will plot the waveform of the target audio and the generated audio\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "librosa.display.waveshow(y1, sr=sr1)\n",
    "plt.title('Waveform of Target Audio')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "librosa.display.waveshow(y2, sr=sr2)\n",
    "plt.title('Waveform of Generated Audio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting sample rate to 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [00:10<00:00, 58.53it/s]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# GENERATE MONO AUDIO OUT OF STEREO SIGNALS\n",
    "###\n",
    "\n",
    "import librosa, os\n",
    "from tqdm import tqdm \n",
    "import soundfile as sf\n",
    "\n",
    "path = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out14_NOCAVP_audio/output\"\n",
    "\n",
    "sr = None\n",
    "for audio in tqdm(os.listdir(path)):\n",
    "    if os.path.isdir(os.path.join(path, audio)):\n",
    "        continue\n",
    "    \n",
    "    if sr is None:\n",
    "        sr = librosa.get_samplerate(os.path.join(path, audio))\n",
    "        print(\"Setting sample rate to\", sr)\n",
    "    y, sr = librosa.load(os.path.join(path, audio), sr=sr, mono=True)\n",
    "\n",
    "    if not os.path.exists(os.path.join(path, \"mono_audio\")):\n",
    "        os.mkdir(os.path.join(path, \"mono_audio\")) \n",
    "    # Save mono wav file\n",
    "    # librosa.output.write_wav(os.path.join(path, \"mono_audio\", audio), y, sr)\n",
    "    sf.write(os.path.join(path, \"mono_audio\", audio), y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# CONVERT AUDIO TO DESIDERED SAMPLE RATE\n",
    "###\n",
    "import os, soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "path = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out/output\"\n",
    "\n",
    "input_sample_rate = 44100\n",
    "desired_sample_rate = 16000\n",
    "\n",
    "for audio in tqdm(os.listdir(path)):\n",
    "    if os.path.isdir(os.path.join(path, audio)):\n",
    "        continue\n",
    "\n",
    "    y, sr = librosa.load(os.path.join(path, audio), sr=input_sample_rate, mono=True)\n",
    "    y = librosa.resample(y, sr, desired_sample_rate)\n",
    "    if not os.path.exists(os.path.join(path, \"convert\", desired_sample_rate)):\n",
    "        os.mkdir(os.path.join(path, \"convert\", desired_sample_rate))\n",
    "    sf.write(os.path.join(path, \"convert\", desired_sample_rate, audio), y, desired_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 605/605 [00:09<00:00, 62.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# CALCOLO E-L1\n",
    "# Please note: here we computed the metric on mono audios \n",
    "# (but computing it on stereo audios should not produce significantly different results)\n",
    "# we suppose 44100 Hz sample rate for all audios\n",
    "\n",
    "# NOT YET OPTIMIZED TO MAKE IT USER FRIENDLY :')\n",
    "\n",
    "import os, soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Your generated results\n",
    "path = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out7_TCCLIP_best_epoch_TEXT/output/mono_audio\"\n",
    "# Inputs\n",
    "path_input = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out/input/mono_audio\"\n",
    "l1_distances = []\n",
    "\n",
    "named_differently = False\n",
    "\n",
    "e_l1 = torch.nn.L1Loss()\n",
    "\n",
    "list_path_input_audio = sorted(list(os.listdir(path_input)))\n",
    "\n",
    "for audio in tqdm(os.listdir(path)):\n",
    "    if os.path.isdir(os.path.join(path, audio)):\n",
    "        continue\n",
    "\n",
    "    y, sr = librosa.load(os.path.join(path, audio), sr=44100, mono=True)\n",
    "    if named_differently:\n",
    "        numero_audio = int(audio.split(\"_\")[0])\n",
    "        filtrato = list(filter(lambda x: \"wav\" in x and int(x.split(\"_\")[0]) == numero_audio, list_path_input_audio))\n",
    "        if len(filtrato) > 1:\n",
    "            print(\"Errore\")\n",
    "            break\n",
    "        y_input, sr_input = librosa.load(os.path.join(path_input, filtrato[0]), sr=44100, mono=True)\n",
    "    else:  \n",
    "        y_input, sr_input = librosa.load(os.path.join(path_input, audio), sr=44100, mono=True)\n",
    "\n",
    "    y_zeros = np.zeros_like(y_input)\n",
    "    y_zeros[:len(y)] = y\n",
    "    y = y_zeros\n",
    "    \n",
    "    rms_y = librosa.feature.rms(y=y, frame_length=512, hop_length=128, pad_mode='reflect')[0]\n",
    "    rms_y_input = librosa.feature.rms(y=y_input, frame_length=512, hop_length=128, pad_mode='reflect')[0]\n",
    "    \n",
    "    rms_y = torch.tensor(rms_y)\n",
    "    rms_y_input = torch.tensor(rms_y_input)\n",
    "\n",
    "    to_take = torch.logical_or(rms_y!=0, rms_y_input!=0)\n",
    "\n",
    "    l1_distance = e_l1(rms_y, rms_y_input)\n",
    "    l1_distances.append(l1_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# GENERATE VIDEOS WITH AUDIO\n",
    "# Take the audio file, the muted video and generates a video with the audio\n",
    "#\n",
    "###\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "# Where to find audios\n",
    "audio_folder = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out6_BNINCEPTION_best_epoch/output/mono_audio\"\n",
    "# Where to find muted videos\n",
    "video_folder = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out/input/video_noaudio\"\n",
    "# Where to save results\n",
    "output_folder = \"/home/christian/syncfusion/Stable-Video2Audio/notebook/out6_BNINCEPTION_best_epoch/output/video\"\n",
    "\n",
    "list_all_audios = list(os.listdir(audio_folder))\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "for video in tqdm(os.listdir(video_folder)):\n",
    "    if os.path.isdir(os.path.join(video_folder, video)):\n",
    "        continue\n",
    "    video_path = os.path.join(video_folder, video)\n",
    "    \n",
    "    num_video = int(video.split(\"_\")[0])\n",
    "    filtrato = list(filter(lambda x: \"wav\" in x and int(x.split(\"_\")[0]) == num_video, list_all_audios))\n",
    "\n",
    "    if len(filtrato) != 1:\n",
    "        print(\"Errore\")\n",
    "        break\n",
    "\n",
    "    audio_path = os.path.join(audio_folder, filtrato[0])\n",
    "    output_path = os.path.join(output_folder, video)\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-i', video_path,\n",
    "        '-i', audio_path,\n",
    "        '-c:v', 'copy',\n",
    "        '-c:a', 'aac',\n",
    "        output_path\n",
    "    ]\n",
    "\n",
    "    # Execute the command\n",
    "    subprocess.run(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stablefoley",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
